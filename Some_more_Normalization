The goal of normalization is to transform features to be on a similar scale. 
This improves the performance and training stability of the model.
Four common normalization techniques may be useful:

1. scaling to a range : convert floating-point feature value from their natural range into standard Range (-1 to +1)
    Formula : Xi=(Xi-Xmean)/(Xmax-Xmin)
    When to use:
        * You know the approximate upper and lower bounds on your data with few or no outliers.
        * Your data is approximately uniformly distributed across that range

    Example : age 
    
2 Clipping : If your data set contains extreme outliers, you might try feature clipping, which caps all feature values above (or below) a certain value to fixed value.\
   For example, you could clip all temperature values above 40 to be exactly 40.

3. Log scaling :  computes the log of your values to compress a wide range to a narrow range.
    Formula : X=Log(Xi)
    When to use :
      Log scaling is helpful when a handful of your values have many points, while most other values have few points. This data distribution is known as the power law distribution.
     Example :
     Movie ratings are a good example. I 
     
     
     
4 Z-score :Z-score is a variation of scaling that represents the number of standard deviations away from the mean. You would use z-score to ensure your feature distributions have mean = 0 and std = 1. Itâ€™s useful when there are a few outliers, but not so extreme that you need clipping.

The formula for calculating the z-score of a point, x, is as follows:
Xi=Xi-Xmean/Standard deviation

